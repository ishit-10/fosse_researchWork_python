We will adopt a two stage methodology that integrates static python program analysis along with 3 LLM driven analytical questioning. First, the student written code sample (we use a simple binary search implementation using python) is subjected to AST (Abstract Syntax Tree) parsing and linter analysis to extract the critical structural properties, such as loop invariants, boundary conditions etc. This gives us an ideal baseline of syntactic and semantic facts about the program independent of various models' interpretations. In the second stage, selected modelsâ€” DeepSeek-V3, Llama 3.1 (405B), and OpenAI GPT-OSS-20B are prompted with the raw student code alongside structured evaluation instructions. Each model is tasked with generating: 
            (a) conceptual prompts examining algorithmic reasoning,
            (b) misconception probes targeting boundary conditions and precondition errors,
            (c) metacognitive prompts encouraging reflection without leaking direct solutions
The outputs are logged and compared against static analysis findings from the AST to assess whether prompts are evidence-grounded and pedagogically aligned. 


Suitability will be assessed across four technical dimensions: grounding fidelity (degree to which prompts explicitly reference constructs like while left < right or missing base cases), conceptual depth (ability to elicit reasoning about invariants, asymptotic complexity, and edge cases) and computational feasibility. 
DeepSeek-V3 is included for its mixture-of-experts architecture and state-of-the-art reasoning ability, hoping for it to capture nuanced boundary-condition errors.
Llama 3.1 is selected for its instruction-tuned design, expected to give chronologically structured and diverse questioning strategies.
GPT-OSS-20B serves as an easily accessible baseline, offering efficient inference on moderate hardware, though it provides less depth in reasoning, it acts as a standard for LLM outputs.
Together, these models offer a well balanced reasoning output that gives us a good idea of how LLMs think.
